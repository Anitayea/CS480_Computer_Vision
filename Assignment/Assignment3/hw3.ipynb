{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec6afc9",
   "metadata": {},
   "source": [
    "# CSCI-UA 0480-042 Computer Vision\n",
    "## Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c14fd",
   "metadata": {},
   "source": [
    "Enter your name and NetID below.\n",
    "\n",
    "#### Name: Anita Ye\n",
    "#### NetID: yy3557"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed58f6a",
   "metadata": {},
   "source": [
    "The main goals of this assignment include:\n",
    "\n",
    "1. Giving an introduction to Mask-RCNN\n",
    "2. Training the predictors for a given dataset\n",
    "3. Finetuning the entire network for the same dataset\n",
    "\n",
    "Also accompanying each part, there are a few questions (**12 questions in total**) -- 11 mandatory + 1 extra credit. The first 11 questions are worth 100 points and the extra credit question is worth 10 points. \n",
    "\n",
    "Please give your answers in the space provided. This homework has a mix of conceptual and coding questions. You can quickly navigate to coding questions by searching (Ctrl/Cmd-F) for `TODO:`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef104bf",
   "metadata": {},
   "source": [
    "# 1. Introduction to Mask-RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc4329",
   "metadata": {},
   "source": [
    "[Mask-RCNN](https://arxiv.org/pdf/1703.06870.pdf) is a network used for instance segmentation. Instance segmentation can be thought of as a hybrid of semantic segmentation and object detection. In other words, we don't want to just find the bounding boxes for each object in our image, we're also interested in finding the segmentation mask of *each object instance*.\n",
    "\n",
    "![Instance Segmentation as a Hybrid of Semantic Segmentation and Object Detection](../shared/HW3/img/instance_segmentation.png)\n",
    "\n",
    "Image Credits: https://towardsdatascience.com/single-stage-instance-segmentation-a-review-1eeb66e0cc49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4a2e9",
   "metadata": {},
   "source": [
    "Mask-RCNN is built on top of Faster-RCNN, which is a network used for object detection. Faster-RCNN has 2 outputs for each candidate object (Region of Interest or RoI) - a class label and a bounding box offset. Mask-RCNN adds a third branch to Faster-RCNN for predicting segmentation masks on each RoI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf97045",
   "metadata": {},
   "source": [
    "We'll first briefly go over Faster-RCNN. Faster-RCNN has 2 stages:\n",
    "\n",
    "1. **Region Proposal Network (RPN):** Given the image, it proposes candidate object bounding boxes. Previous object detection models such as RCNN and Fast-RCNN handled this separately from the CNN model. Faster-RCNN takes a different approach -- it integrates these two components into the same network to achieve speedup.\n",
    "2. **Fast-RCNN:** This stages takes each candidate RoI and extracts features from the image feature vector using RoIPool. Using these RoI features, it performs classification and bounding box regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca162556",
   "metadata": {},
   "source": [
    "![Mask-RCNN framework for instance segmentation](../shared/HW3/img/mask_rcnn_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf472160",
   "metadata": {},
   "source": [
    "Mask-RCNN has the same 2-stage procedure, but in the 2nd stage, instead of just predicting the classification label and the bounding box offset, it predicts **in parallel** a binary mask for each RoI. \n",
    "\n",
    "Mask-RCNN relies on a pretrained network (called the \"backbone\" in the paper) to extract features from the image. These features are fed into the Region Proposal Network (RPN) to generate candidate RoIs. For each RoI candidate, a fixed size RoI feature vector is generated using an RoIAlign layer. This RoI feature map is then provided to the classifier, bounding box predictor and the segmentation mask to generate the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b10c1d",
   "metadata": {},
   "source": [
    "## Question 1 [10%]\n",
    "\n",
    "Training uses a multi-task loss function. What are the three components in this loss function? Is the loss computed per image or per RoI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026877f7",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "Multi-task loss function on each sampled RoI is $L = L_{cls} + L_{box}+L_{mask}$, which are classfication loss, bounding-box(localization) loss and mask loss. \n",
    "\n",
    "Loss is computed per RoI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f35db5",
   "metadata": {},
   "source": [
    "## Question 2 [5%]\n",
    "[This blog post](https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html) by Lilian Weng gives a nice overview of the object detection (RCNN type) networks. In the blog post, it is mentioned that Mask-RCNN uses RoIAlign instead of RoIPool. Explain briefly in 3-4 lines why this is being done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d3eb3",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "RoIPool quantizes a floating-point RoI to match the feature map's discrete grid, and then it subdivides the RoI into spatial bins, which are also quantized. Quantization process can cause misalignments between the RoI and the extracted features, and when predicting pixel-accurate masks, these misalignments can significantly degrade performance. To address this issue, we use RoIAlign instead of RoIPool since it avoids quantization and employs bilinear interpolation to ensure that the extracted features align accurately with the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9c923",
   "metadata": {},
   "source": [
    "## Question 3 [5%]\n",
    "What are the different backbones explored in the Mask-RCNN paper? They are denoted in the paper using network-depth-features nomenclature. What is the advantage of using a ResNet-FPN backbone over a ResNet-C4 backbone for feature extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ead535",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "FPN and C4 are backbones explored in the Mask-RCNN paper. We can see that deeper networks do better, FPN outerforms C4, and ResNeXt improves on ResNet. \n",
    "\n",
    "ResNet-C4 uses the basic ResNet architecture up to the fourth stage, while ResNet-FPN adds a Feature Pyramid Network to ResNet, enabling the model to capture features at multiple scales.\n",
    "\n",
    "The advantage of ResNet-FPN over ResNet-C4 is its ability to handle objects of different sizes effectively, because of its multi-scale feature representation, improved localization, better handling of small objects, and enhanced semantic understanding. It's particularly beneficial for tasks like instance segmentation with varying object sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fddd49",
   "metadata": {},
   "source": [
    "# 2. Training the Predictors for a New Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d1e60",
   "metadata": {},
   "source": [
    "In this section, we'll start with a pretrained Mask-RCNN model that uses Resnet-50-FPN as the backbone. This model was trained on MS-COCO dataset which is widely used for multiple vision tasks such as object detection, instance segmentation, etc.\n",
    "\n",
    "MS-COCO has 91 classes (90 for objects + 1 for background). Some sample objects in the dataset include `person`, `car`, `bicycle`, `knife`, `train`, etc. \n",
    "\n",
    "Along with this homework file, we have also provided another sample dataset (we'll refer to it as the [Nature dataset](https://towardsdatascience.com/custom-instance-segmentation-training-with-7-lines-of-code-ff340851e99b)). It isn't a standard dataset, but it's small enough (600 train + 200 test images) and allows us to easily demo finetuning a pretrained Mask-RCNN model. This dataset contains only 2 classes - `squirrel` and `butterfly`. \n",
    "\n",
    "Our goal in part2 and part3 of this assignment is to take the pretrained Mask-RCNN model and finetune/train it for this dataset. However, here in part2, instead of finetuning the entire network, we'll train only the final layers.\n",
    "\n",
    "In Homework 2, we've shown how one could feed data into the network using `Dataset`s and `DataLoader`s. We'll use the same strategy here for finetuning the model.\n",
    "\n",
    "We've based this homework on this [PyTorch tutorial on Object Detection Finetuning](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edb777",
   "metadata": {},
   "source": [
    "## Question 4 [25%]\n",
    "\n",
    "Complete the TODO section in below cell. Specifically:\n",
    "\n",
    "1. Read image height, width and polygon shapes data from the JSON file.\n",
    "2. Generate 2D masks from the polygon points. You can follow this idea: https://stackoverflow.com/a/3732128\n",
    "3. Generate the bounding boxes from the mask data. Assume that the bounding box is a rectangle with the smallest area enclosing the mask.\n",
    "\n",
    "You may find this json schema useful:\n",
    "```\n",
    "{\n",
    "   \"shapes\": [ # list of object instances; masks are represented as polygons\n",
    "       ## data for instance1 \n",
    "       {\n",
    "           \"label\": \"\" # label for instance1\n",
    "           \"points\": [] # 2d list of polygon points [(x1, y1), (x2, y2), ..]\n",
    "       },\n",
    "       ## data for instance2\n",
    "       {\n",
    "           \"label\": []\n",
    "           \"points\": []\n",
    "       },\n",
    "       \n",
    "       ..\n",
    "       ..\n",
    "   ],\n",
    "   \"imagePath\": \"\"\n",
    "   \"imageData\" : \"\"\n",
    "   \"imageHeight\": <integer>\n",
    "   \"imageWidth\": <integer>\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you could also uncomment the line below to see a sample json file\n",
    "!cat '../shared/data/HW3/nature/train/s (3).json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import json # for reading from json file\n",
    "import glob # for listing files inside a folder\n",
    "from PIL import Image, ImageDraw # for reading images and drawing masks on them.\n",
    "\n",
    "\n",
    "# Create a custom dataset class for Nature\n",
    "# dataset subclassing PyTorch's Dataset class\n",
    "class NatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load all image files, sorting them to\n",
    "        # ensure that they are aligned with json files\n",
    "        imgs = glob.glob(root + '/*.jpg')\n",
    "        imgs += glob.glob(root + '/*.png') # some images are in png format\n",
    "        self.imgs = sorted(imgs)\n",
    "        \n",
    "        # Mask data is stored in a json file\n",
    "        masks = glob.glob(root + '/*.json')\n",
    "        self.masks = sorted(masks)\n",
    "\n",
    "        # Each image can have multiple object instances, and each\n",
    "        # instance is associated with either of these 2 labels.\n",
    "\n",
    "        # Need to convert str-labels to ids. So we'll use \n",
    "        # this label-to-index mapping.\n",
    "        # Note: we can't start from 0 because 0 is restricted\n",
    "        # to the \"background\" class\n",
    "        self.label_to_id = {'squirrel': 1, 'butterfly': 2}\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Have already aligned images and JSON files; can now\n",
    "        # simply use the index to access both images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        \n",
    "        # Read image using PIL.Image and convert it to an RGB image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # TODO: Read image height, width and mask data from\n",
    "        # the JSON file\n",
    "        with open(mask_path, 'r') as fp:\n",
    "            # TODO: Using json library read the dictionary\n",
    "            # from the fp\n",
    "            json_dict = json.load(fp)\n",
    "            \n",
    "            # TODO:\n",
    "            height = json_dict[\"imageHeight\"]\n",
    "            \n",
    "            # TODO:\n",
    "            width = json_dict[\"imageWidth\"]\n",
    "            \n",
    "            # TODO:\n",
    "            poly_shapes_data = json_dict[\"shapes\"]\n",
    "        \n",
    "        # TODO: Each image can have multiple mask instances.\n",
    "        # Using the polygon points, generate the 2d-mask\n",
    "        # using PIL's ImageDraw.polygon\n",
    "        masks = []\n",
    "        labels = []\n",
    "        for shape_data in poly_shapes_data:\n",
    "            polygon_points = [tuple(point) for point in shape_data['points']]\n",
    "            \n",
    "            # TODO: Using Image.new() create an image of size (width, height)\n",
    "            # and fill it with 0s.\n",
    "            mask_img = Image.new('L', (width, height), 0)\n",
    "            \n",
    "            # TODO: Draw the mask on the base image we just created\n",
    "            ImageDraw.Draw(mask_img).polygon(polygon_points, outline=1, fill=1)\n",
    "\n",
    "            mask = np.array(mask_img)\n",
    "            masks.append(mask)\n",
    "\n",
    "            label = shape_data['label']\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Each mask instance also has an associated label which is str-type\n",
    "        # Convert the str into an int using the mapping we created in __init__\n",
    "        labels = [self.label_to_id[label] for label in labels]\n",
    "        \n",
    "        # TODO: Generate the bounding boxes for each instance\n",
    "        # from the 2d masks\n",
    "        num_objs = len(masks)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            # TODO: Use np.where() to find where mask[i] == True.\n",
    "            # pos will be a 2d-list of indices\n",
    "            pos = np.where(masks[i] == 1)\n",
    "            \n",
    "            # In pos, find the min x- and y- indices;\n",
    "            # max x- and y- indices. This will give us our box bounds.\n",
    "            \n",
    "            # TODO:\n",
    "            xmin = np.min(pos[1])\n",
    "            \n",
    "            # TODO:\n",
    "            xmax = np.max(pos[1])\n",
    "            \n",
    "            # TODO:\n",
    "            ymin = np.min(pos[0])\n",
    "            \n",
    "            # TODO:\n",
    "            ymax = np.max(pos[0])\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # Assume all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6311c",
   "metadata": {},
   "source": [
    "Having implemented our `NatureDataset` class, let's create the `Dataset` and the `DataLoader` objects. Note that we're not using `torchvision.transforms`, instead we're using transforms provided in a separate script in this directory called `transforms.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = NatureDataset('../shared/data/HW3/nature/train', get_transform(train=True))\n",
    "dataset_test = NatureDataset('../shared/data/HW3/nature/test', get_transform(train=False))\n",
    "\n",
    "import utils\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b9aa2",
   "metadata": {},
   "source": [
    "Now let's visualize one image from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, targets = dataset[506]\n",
    "\n",
    "# np.transpose docs: https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
    "# img is a PyTorch tensor, can convert it to a NumPy tensor by calling .numpy() on it.\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374990f1",
   "metadata": {},
   "source": [
    "And here's the corresponding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035782e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(targets['masks'].numpy().squeeze(), interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a9b8d",
   "metadata": {},
   "source": [
    "We'll be training the final layers of the pretrained Mask-RCNN model (with Resnet-50-FPN backbone) available in the `torchvision` package. So let's download the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p2 = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0053ac",
   "metadata": {},
   "source": [
    "## Question 5 [5%]\n",
    "\n",
    "Recall what we said earlier: For training for the new dataset, we need to modify its box predictor (FastRCNNPredictor) and its mask predictor (MaskRCNNPredictor) to match with the new dataset. Complete the code cell below.\n",
    "\n",
    "You may find these docs for `FastRCNNPredictor` and `MaskRCNNPredictor` useful:\n",
    "\n",
    "![FastRCNNPredictor Docs](../shared/HW3/img/FastRCNNPredictor_docs.png)\n",
    "![MaskRCNNPredictor Docs](../shared/HW3/img/MaskRCNNPredictor_docs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90221499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Our new dataset has 3 classes: butterfly, squirrel and background\n",
    "num_classes = 3\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model_p2.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# TODO: replace the pre-trained head with a new one\n",
    "model_p2.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Get number of input features for the mask predictor\n",
    "in_features_mask = model_p2.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "\n",
    "# TODO: replace the mask predictor with a new one\n",
    "model_p2.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f737fc",
   "metadata": {},
   "source": [
    "Training these layers can take several minutes on a CPU, so we've provided GPUs to make this faster. It should take ~5 mins to run the training in Q7. But before that, we want to ensure that PyTorch is able to access the GPU by printing the device PyTorch is currently (prints `cuda` if it's using a GPU, otherwise it prints `cpu`). \n",
    "\n",
    "Now we want to freeze all the layers below these predictors. We can do this by setting the `.requires_grad` attribute of the parameters we want to freeze to `False`. Read more about `requires_grad` from [this PyTorch page on Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs-from-backward).\n",
    "\n",
    "In order to do the computation on a GPU, we have to move the model from main memory to GPU memory. This can be done by simply calling `.to(device)` on the model. See [the docs](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24ddfb",
   "metadata": {},
   "source": [
    "### Note : Make sure the below prints `Using : cuda` before proceeding further. If it doesn't print cuda , make sure you are in the GPU profile of the server. To start the server in GPU profile, shutdown the current server and restart the server by selecting the GPU option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Freeze model and move it to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using : \", device)\n",
    "\n",
    "for param in model_p2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "pred_params = itertools.chain(\n",
    "    model_p2.roi_heads.mask_predictor.parameters(),\n",
    "    model_p2.roi_heads.box_predictor.parameters()\n",
    ")\n",
    "\n",
    "for param in pred_params:\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_p2 = model_p2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99668a06",
   "metadata": {},
   "source": [
    "We were able to verify that PyTorch is able to access a GPU. Now let's see the layers inside the `model_p2.roi_heads` to understand what we have modified here (we just modified `box_predictor` and `mask_predictor`). You could also verify the output below from figure 4 in the Mask-RCNN paper. You'll notice that it's the exact same network on the right part of that figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3828f",
   "metadata": {},
   "source": [
    "We can use below command to check what are the GPU devices that are available to us and their current memory usage. Having a GPU in the list doesn't mean we can simply use it for training. We need to have proper CUDA drivers and compatible versions of pytorch/torchvision libraries. We have made sure that the versions are compatible and one is able to use GPU without and additional changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p2.roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13848f71",
   "metadata": {},
   "source": [
    "## Question 6 [10%]\n",
    "We just printed the head architecture. From the above output, list all the layers we're training along their names. \n",
    "\n",
    "For example, if we're training mask_fcn1 of mask_head you'll specify:\n",
    "\n",
    "`mask_head.mask_fcn1`: 2d-Conv layer with 256 input channels, 256 output channels and kernel size = (2, 2).\n",
    "\n",
    "Note: We're **only** asking for layers with trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4dd28",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "mask_predictor.conv5_mask: 2d-Conv layer, input channel = 256, output channels = 256, kernel size = (2,2), stride = (2,2).\n",
    "\n",
    "mask_predictor.mask_fcn_logits: 2d-Conv layer, input channel = 256, output channels = 3, kernel size = (2,2).\n",
    "\n",
    "box_predictor.cls_score: linear layer with 1024 input features, 3 output features.\n",
    "\n",
    "box_predictor.bbox_pred: linear layer with 1024 input features, 12 output features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205efab7",
   "metadata": {},
   "source": [
    "Both the dataloaders and model have been prepared for training. All that remains is to set an optimizer and a learning rate scheduler. When we create an optimizer, we have to provide it the list of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare optimizer and lr\n",
    "params = [p for p in model_p2.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce045f35",
   "metadata": {},
   "source": [
    "## Question 7 [10%]\n",
    "\n",
    "How many trainable parameters are we passing to the optimizer?\n",
    "\n",
    "Here's an example to calculate # of trainable parameters in a fully connected layer with:\n",
    "1. an additive bias\n",
    "2. in_channels = 1024\n",
    "3. out_channels = 10\n",
    "\n",
    "The number of trainable parameters here will be 1024\\*10 + 10 = 10250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7435d",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "box_predictor.cls_score: 1024x3+3=3075.\n",
    "\n",
    "box_predictor.bbox_pred: 1024x12+12=12300.\n",
    "\n",
    "mask_predictor.conv5_mask: ((256x2x2)+1)x256=262400.\n",
    "\n",
    "mask_predictor.mask_fcn_logits: ((256x1x1)+1)x3=771.\n",
    "\n",
    "3075+12300+262400+771=278546."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce08b74",
   "metadata": {},
   "source": [
    "Now we can finally start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\", epoch)\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model_p2, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model_p2, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb139085",
   "metadata": {},
   "source": [
    "In order to analyze the output, you'll need to know what an IoU score is. You can read this blog: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b0c4f",
   "metadata": {},
   "source": [
    "## Question 8 [10%]\n",
    "\n",
    "In an image (grey box) of size 12x16, there is an object whose ground truth mask is the box with the dashed edge (green color) and the model's predicted mask is the box with the dash-dotted edge (red color). Calculate the IoU score for this prediction.\n",
    "\n",
    "![Image](../shared/HW3/img/plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5609a2a",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "$$\n",
    "IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n",
    "$$\n",
    "\n",
    "Area of intersection = 5 * 4 = 20\n",
    "\n",
    "Area of Union = 35 + 25 - 20 = 40\n",
    "\n",
    "ROI = 20 / 40 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94136f1",
   "metadata": {},
   "source": [
    "Let's see the model's prediction for a sample from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d33038",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset_test[1]\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model_p2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model_p2([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d04ae",
   "metadata": {},
   "source": [
    "Here's the sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e691f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ea9f6",
   "metadata": {},
   "source": [
    "And here's the mask generated by the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267af2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ba4ab",
   "metadata": {},
   "source": [
    "# 3. Finetuning the Entire Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a139008",
   "metadata": {},
   "source": [
    "Let's see what happens when we fine-tune the entire network. That is, instead of just learning the weights in the final layers, we'll let the weights of the entire network change during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p3 = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e30c92",
   "metadata": {},
   "source": [
    "## Question 9 [5%]\n",
    "\n",
    "Just like in question 5, we're interested in fine-tuning the pretrained model for our new dataset, so we will again need to modify its box predictor (FastRCNNPredictor) and its mask predictor (MaskRCNNPredictor) to match with our new dataset. Complete the code cell below. Hint: This is exactly the same as question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e959a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Our new dataset has 3 classes: butterfly, squirrel and background\n",
    "num_classes = 3\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model_p3.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# TODO: replace the pre-trained head with a new one\n",
    "model_p3.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Get number of input features for the mask predictor\n",
    "in_features_mask = model_p3.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "\n",
    "# TODO: replace the mask predictor with a new one\n",
    "model_p3.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd1749",
   "metadata": {},
   "source": [
    "Again, let's ensure that we're using the GPU by printing the device info. This finetuning process takes even longer because we have more trainable parameters, hence there will be more computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using : \", device)\n",
    "\n",
    "model_p3 = model_p3.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21d258",
   "metadata": {},
   "source": [
    "We'll use the same optimizer and learning rate scheduler as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b88212",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model_p3.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f0ebe",
   "metadata": {},
   "source": [
    "Let's start the finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\", epoch)\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model_p3, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model_p3, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7b7b9",
   "metadata": {},
   "source": [
    "We'll generate the output for the same image, but this time with the new finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset_test[1]\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model_p3.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model_p3([img.to(device)])\n",
    "\n",
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0164d3",
   "metadata": {},
   "source": [
    "Here's the predicted image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21333d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399d5b2",
   "metadata": {},
   "source": [
    "## Question 10 [5%]\n",
    "\n",
    "Does this model perform better than the trained model after Q7 ? How do you campare these models ? Explain why the model performs better than the other model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc90503",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "This model performed better than the trained model after Q7. \n",
    "\n",
    "To assess their performance, compared these two metrics: Average Precision (AP) and Average Recall (AR). AP measures the ratio of True Positives to the sum of True Positives and False Positives, while AR measures the ratio of True Positives to the sum of True Positives and False Negatives. Higher values of AP and AR indicate better performance. Here, across various Intersection over Union (IoU) thresholds ranging from 0.50 to 0.95, which outperforms the previous model in terms of both AP and AR.\n",
    "\n",
    "The key to this model's superior performance lies in its training approach. Unlike the previous model, which only updated the weights in the final layers, this model allows the entire network's weights to adapt during training. This comprehensive fine-tuning process contributes to its improved results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486bf62",
   "metadata": {},
   "source": [
    "## Question 11 [10 %]\n",
    "\n",
    "Discuss any 4 advantage of using CONV layers over FC/Dense layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc750fd",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "Parameter Efficiency: Convolutional layers (CONV) with strides >1 typically use fewer parameters compared to Fully Connected (FC) layers.\n",
    "\n",
    "Semantic Feature Maps: CONV layers produce feature maps containing semantic information extracted from the original image.\n",
    "\n",
    "Preservation of Spatial Relationships: CONV layers retain spatial relationships within the image to some extent, while FC layers treat each pixel as an isolated node.\n",
    "\n",
    "Suitability for Image Tasks: CONV layers are well-suited for image-related tasks and often outperform FC layers in classification tasks on datasets like CIFAR-10 or MNIST.\n",
    "\n",
    "Reduced Overfitting: CONV layers are less prone to overfitting, which occurs when a model becomes overly specialized on a particular training dataset and performs poorly on other test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb3b09",
   "metadata": {},
   "source": [
    "# 4. Extra Credit Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbbed9",
   "metadata": {},
   "source": [
    "## Question 12 [10 points]\n",
    "\n",
    "Can fully convolutional networks (FCN) be used for object detection? In Mask-RCNN we have 3 branches — mask, classification, and bounding box regression — out of which the last 2 have fully connected (FC) layers. Can this entire pipeline be replaced by a fully convolutional network? If possible, discuss any 2 networks to support your claim explaining their architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a79487",
   "metadata": {},
   "source": [
    "#### Answer :\n",
    "\n",
    "Yes, Fully Convolutional Networks (FCN) can be employed for object detection tasks. FCNs have the potential to replace the entire object detection pipeline.\n",
    "\n",
    "Fully Convolutional One-Stage Object Detection (FSCO) and CornerNet adopt anchor-free frameworks to transform FCN into object detectors. These methods detect objects by considering them as paired keypoints, offering alternative strategies for object detection. Also YOLOv3 is with convolutional layers. U-net is also a great example with convolutional layers and ReLu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csciua-0480042-fall]",
   "language": "python",
   "name": "conda-env-csciua-0480042-fall-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
